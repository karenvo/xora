"""Generate standalone password generation scripts for a target."""

from __future__ import annotations

import hashlib
import json
import re
import textwrap
from collections import Counter
from datetime import datetime
from pathlib import Path

from xora.pattern_analyzer import PatternAnalysis
from xora.profile_parser import TargetProfile

# ============================================================================
# SCRIPT SKELETON — boilerplate shared by both LLM and fallback scripts.
# Contains: imports, data constants, utility functions (leet_variants,
# case_variants, policy, scoring, output formatting), argparse, main().
#
# The generation engine (generate_all + helpers) is injected separately
# via {generation_engine}.
# ============================================================================

_SCRIPT_SKELETON = textwrap.dedent('''\
    #!/usr/bin/env python3
    """Password candidate generator for target: {target_name}
    Generated by xora v{version} on {timestamp}
    Mode: {generation_mode}

    Usage:
        python3 {script_name}                          # dump all to stdout
        python3 {script_name} --ranked                 # sort by likelihood
        python3 {script_name} --limit 500              # cap output
        python3 {script_name} --min-length 10          # override policy
        python3 {script_name} --format hydra           # user:pass pairs
        python3 {script_name} -o wordlist.txt          # write to file

    Pipe directly into attack tools:
        python3 {script_name} | hydra -l user -P /dev/stdin ssh://target
        python3 {script_name} | hashcat -m 1000 hashes.txt
    """

    import argparse
    import itertools
    import re
    import sys
    from typing import Iterator

    # ========================================================================
    # PROFILE DATA — tiered by password likelihood
    # ========================================================================

    WORD_TIERS = {word_tiers}

    NUMBERS = {numbers}

    KNOWN_PASSWORDS = {known_passwords}

    EMAILS = {emails}

    USERNAMES = {usernames}

    # ========================================================================
    # DETECTED PATTERNS — from known password analysis
    # ========================================================================

    CAP_STYLE = "{cap_style}"
    AVG_LENGTH = {avg_length}
    PATTERN_TEMPLATES = {pattern_templates}

    # ========================================================================
    # PASSWORD PSYCHOLOGY PROFILE
    # ========================================================================

    CATEGORY_WEIGHTS = {category_weights}

    # ========================================================================
    # SEPARATOR FINGERPRINT
    # ========================================================================

    PREFERRED_SEPARATORS = {preferred_separators}
    RARE_SEPARATORS = {rare_separators}

    # ========================================================================
    # PASSWORD STRENGTH PROFILE
    # ========================================================================

    STRENGTH_TIER = "{strength_tier}"
    STRENGTH_AVG_SCORE = {strength_avg_score}
    STRENGTH_REUSE_RATIO = {strength_reuse_ratio}
    STRENGTH_LEET_PCT = {strength_leet_pct}
    STRENGTH_ALWAYS_SPECIAL = {strength_always_special}
    STRENGTH_ALWAYS_DIGIT = {strength_always_digit}
    STRENGTH_ALWAYS_UPPER = {strength_always_upper}

    # ========================================================================
    # LLM-GENERATED CANDIDATES — psychologically targeted by AI
    # Model: {llm_model} | Generated: {timestamp}
    # ========================================================================

    LLM_CANDIDATES = {llm_candidates}

    # ========================================================================
    # GLUE WORDS & SEMANTIC DATA
    # ========================================================================

    GLUE_WORDS = {glue_words}
    SEMANTIC_TEMPLATES = {semantic_templates}
    ROLE_VOCABULARY = {role_vocabulary}

    # ========================================================================
    # PASSWORD POLICY
    # ========================================================================

    DEFAULT_POLICY = {{
        "min_length": {min_length},
        "max_length": {max_length},
        "require_upper": {require_upper},
        "require_lower": {require_lower},
        "require_digit": {require_digit},
        "require_special": {require_special},
    }}

    # ========================================================================
    # LEET SPEAK MAP — built from target's actual substitution patterns
    # ========================================================================

    LEET_MAP = {leet_map}

    # ========================================================================
    # UTILITY FUNCTIONS
    # ========================================================================

    def _all_words() -> list[str]:
        """Flatten WORD_TIERS into a single deduped list, critical first."""
        seen = set()
        words = []
        for tier in ["critical", "high", "medium", "low"]:
            for w in WORD_TIERS.get(tier, []):
                if w not in seen:
                    seen.add(w)
                    words.append(w)
        return words


    def case_variants(word: str) -> list[str]:
        """Generate capitalization variants biased by detected style."""
        if CAP_STYLE == "capitalize":
            results = [word.capitalize(), word.lower(), word]
        elif CAP_STYLE == "upper":
            results = [word.upper(), word.capitalize(), word]
        elif CAP_STYLE == "lower":
            results = [word.lower(), word.capitalize(), word]
        else:
            results = [word.capitalize(), word.lower(), word.upper(), word]
        return list(dict.fromkeys(results))


    def leet_variants(word: str, max_variants: int = 5) -> list[str]:
        """Apply leet speak substitutions matching the target's style."""
        results = [word]
        lower = word.lower()
        for char, replacements in LEET_MAP.items():
            if char not in lower:
                continue
            new_results = []
            primary = replacements[0]
            secondaries = replacements[1:]
            for current in results[:max_variants]:
                new_results.append(current.replace(char, primary, 1))
                new_results.append(current.replace(char.upper(), primary, 1))
            for rep in secondaries:
                for current in results[:2]:
                    new_results.append(current.replace(char, rep, 1))
                    new_results.append(current.replace(char.upper(), rep, 1))
            results.extend(new_results)
        return list(set(results))[:max_variants * 3]


    def _weighted_seps(seps: list[str], total: int = 6) -> list[str]:
        """Expand separator list weighted by preference order."""
        if not seps:
            return ["!", "_", "#"]
        if len(seps) == 1:
            return seps
        result: list[str] = []
        remaining = total
        for i, s in enumerate(seps):
            if i == 0:
                count = max(1, (total + 1) // 2)
            elif i == 1:
                count = max(1, (remaining + 1) // 2)
            else:
                count = max(1, remaining // max(1, len(seps) - i))
            count = min(count, remaining)
            result.extend([s] * count)
            remaining -= count
            if remaining <= 0:
                break
        return result


    def number_suffixes() -> list[str]:
        """Number suffixes — profile numbers first, then common ones."""
        suffixes = list(NUMBERS)
        for n in NUMBERS:
            if len(n) == 4:
                short = n[2:]
                if short not in suffixes:
                    suffixes.append(short)
        for n in ["1", "12", "123", "1234", "01", "007", "69", "420"]:
            if n not in suffixes:
                suffixes.append(n)
        current_year = 2026
        for y in range(current_year - 5, current_year + 2):
            for s in [str(y), str(y)[2:]]:
                if s not in suffixes:
                    suffixes.append(s)
        return suffixes


    def passes_policy(password: str, policy: dict) -> bool:
        """Check if a password meets the configured policy."""
        if len(password) < policy["min_length"]:
            return False
        if len(password) > policy["max_length"]:
            return False
        if policy["require_upper"] and not any(c.isupper() for c in password):
            return False
        if policy["require_lower"] and not any(c.islower() for c in password):
            return False
        if policy["require_digit"] and not any(c.isdigit() for c in password):
            return False
        if policy["require_special"] and all(c.isalnum() for c in password):
            return False
        return True


    def _segment(password: str) -> str:
        """Structural fingerprint like 'WSWN'."""
        out = []
        prev = ""
        for c in password:
            if c.isalpha():
                kind = "W"
            elif c.isdigit():
                kind = "N"
            else:
                kind = "S"
            if kind != prev:
                out.append(kind)
                prev = kind
        return "".join(out)


    _TEMPLATE_FINGERPRINTS: list[str] = []
    _LLM_LOWER: set[str] = {{p.lower() for p in LLM_CANDIDATES}}


    def _init_fingerprints() -> None:
        if _TEMPLATE_FINGERPRINTS:
            return
        for tmpl in PATTERN_TEMPLATES:
            fp = ""
            for m in re.finditer(r"\\{{(\\w+)\\}}", tmpl):
                kind = m.group(1)
                if kind.startswith("word"):
                    fp += "W"
                elif kind.startswith(("year", "num")):
                    fp += "N"
                elif kind in ("special",):
                    fp += "S"
            if fp:
                _TEMPLATE_FINGERPRINTS.append(fp)


    def score_candidate(password: str) -> float:
        """Score a password by structural match, word tiers, separators, etc."""
        score = 0.3
        _init_fingerprints()

        pw_fp = _segment(password)
        best_struct = 0.0
        for tmpl_fp in _TEMPLATE_FINGERPRINTS:
            if pw_fp == tmpl_fp:
                best_struct = 0.3
                break
            elif tmpl_fp.startswith(pw_fp) or pw_fp.startswith(tmpl_fp):
                best_struct = max(best_struct, 0.15)
        score += best_struct

        lower_pw = password.lower()
        tier_bonuses = {{"critical": 0.25, "high": 0.15, "medium": 0.08, "low": 0.03}}
        best_tier_bonus = 0.0
        for tier_name, bonus in tier_bonuses.items():
            for w in WORD_TIERS.get(tier_name, []):
                if w.lower() in lower_pw:
                    best_tier_bonus = max(best_tier_bonus, bonus)
                    break
            if best_tier_bonus >= 0.25:
                break
        score += best_tier_bonus

        specials_in_pw = [c for c in password if not c.isalnum()]
        if specials_in_pw:
            pref_set = set(PREFERRED_SEPARATORS)
            rare_set = set(RARE_SEPARATORS)
            all_preferred = all(c in pref_set or c in rare_set for c in specials_in_pw)
            if all_preferred:
                if all(c in pref_set for c in specials_in_pw):
                    score += 0.10
                else:
                    score += 0.05
            else:
                score -= 0.05

        if AVG_LENGTH > 0:
            diff = abs(len(password) - AVG_LENGTH)
            if diff <= 2:
                score += 0.10
            elif diff <= 5:
                score += 0.05

        if lower_pw in _LLM_LOWER:
            score += 0.15

        if GLUE_WORDS:
            for gw in GLUE_WORDS:
                if gw.lower() in lower_pw:
                    score += 0.08
                    break

        if len(specials_in_pw) > 2:
            score -= 0.1

        return min(max(score, 0.0), 1.0)


    # ========================================================================
    # GENERATION ENGINE — injected by xora (LLM-written or fallback)
    # ========================================================================

    {generation_engine}


    # ========================================================================
    # OUTPUT & CLI
    # ========================================================================

    def format_output(scored: list[tuple[float, str]], fmt: str,
                      username: str = "") -> Iterator[str]:
        """Format output for different attack tools."""
        if fmt == "hydra":
            user = username or (EMAILS[0].split("@")[0] if EMAILS else "user")
            for _, pw in scored:
                yield f"{{user}}:{{pw}}"
        elif fmt == "credstuff":
            for email in (EMAILS or ["user@example.com"]):
                for _, pw in scored:
                    yield f"{{email}}:{{pw}}"
        elif fmt == "burp":
            seen = set()
            for _, pw in scored:
                if pw not in seen:
                    seen.add(pw)
                    yield pw
        else:
            for _, pw in scored:
                yield pw


    def main() -> None:
        parser = argparse.ArgumentParser(
            description=f"xora password generator for: {target_name}",
            formatter_class=argparse.RawDescriptionHelpFormatter,
        )
        parser.add_argument("--ranked", action="store_true",
                            help="sort by likelihood (most probable first)")
        parser.add_argument("--limit", type=int, default=0,
                            help="max number of passwords to output")
        parser.add_argument("--min-length", type=int, default=None,
                            help="override minimum password length")
        parser.add_argument("--max-length", type=int, default=None,
                            help="override maximum password length")
        parser.add_argument("--require-upper", action="store_true", default=None,
                            help="require uppercase letter")
        parser.add_argument("--no-require-upper", action="store_true",
                            help="don't require uppercase letter")
        parser.add_argument("--require-lower", action="store_true", default=None,
                            help="require lowercase letter")
        parser.add_argument("--no-require-lower", action="store_true",
                            help="don't require lowercase letter")
        parser.add_argument("--require-digit", action="store_true", default=None,
                            help="require digit")
        parser.add_argument("--no-require-digit", action="store_true",
                            help="don't require digit")
        parser.add_argument("--require-special", action="store_true", default=None,
                            help="require special characters")
        parser.add_argument("--no-require-special", action="store_true",
                            help="don't require special characters")
        parser.add_argument("--format", choices=["plain", "hydra", "burp", "credstuff"],
                            default="plain", help="output format (default: plain)")
        parser.add_argument("--username", default="",
                            help="username for hydra/credstuff format")
        parser.add_argument("-o", "--output", default=None,
                            help="write to file instead of stdout")
        args = parser.parse_args()

        policy = dict(DEFAULT_POLICY)
        if args.min_length is not None:
            policy["min_length"] = args.min_length
        if args.max_length is not None:
            policy["max_length"] = args.max_length
        for _fld in ("upper", "lower", "digit", "special"):
            _no = getattr(args, f"no_require_{{_fld}}", False)
            _yes = getattr(args, f"require_{{_fld}}", None)
            if _no:
                policy[f"require_{{_fld}}"] = False
            elif _yes:
                policy[f"require_{{_fld}}"] = True

        total_words = sum(len(v) for v in WORD_TIERS.values())
        print(f"[xora] Target: {target_name}", file=sys.stderr)
        print(f"[xora] Mode: {generation_mode}", file=sys.stderr)
        print(f"[xora] Words: {{total_words}} | LLM candidates: {{len(LLM_CANDIDATES)}}", file=sys.stderr)
        print(f"[xora] Separators: preferred={{PREFERRED_SEPARATORS}} rare={{RARE_SEPARATORS}}",
              file=sys.stderr)
        print(f"[xora] Strength: {{STRENGTH_TIER}} (leet: {{STRENGTH_LEET_PCT:.0%}})",
              file=sys.stderr)

        scored = generate_all(policy)

        print(f"[xora] Candidates generated: {{len(scored):,}}", file=sys.stderr)

        output_lines = list(format_output(scored, args.format, args.username))
        if args.limit:
            output_lines = output_lines[:args.limit]

        if args.output:
            with open(args.output, "w") as f:
                f.write("\\n".join(output_lines) + "\\n")
            print(f"[xora] Written to {{args.output}}", file=sys.stderr)
        else:
            print("\\n".join(output_lines))


    target_name = "{target_name}"

    if __name__ == "__main__":
        main()
''')

# ============================================================================
# FALLBACK ENGINE — used when no LLM is available.
# Simpler combinatorial approach: expand words with case/leet/numbers/seps.
# No blind permutation explosion. Clear that this is basic mode.
# ============================================================================

_FALLBACK_ENGINE = textwrap.dedent('''\
    def generate_all(policy: dict | None = None) -> list[tuple[float, str]]:
        """Generate candidates using basic combinatorial expansion.

        This is the fallback engine (no LLM available). It expands words from
        the tiered word pool with case variants, leet, numbers, and separators.
        For better results, rerun with an LLM: xora analyze --llm ollama
        """
        pol = policy or DEFAULT_POLICY
        nums = number_suffixes()
        pref_seps = PREFERRED_SEPARATORS or ["!", "_", "#"]
        wseps = _weighted_seps(pref_seps, total=6)
        all_words = _all_words()
        leet_pct = STRENGTH_LEET_PCT

        seen: set[str] = set()
        scored: list[tuple[float, str]] = []

        def _add(pw: str) -> None:
            if pw in seen or " " in pw:
                return
            seen.add(pw)
            if passes_policy(pw, pol):
                scored.append((score_candidate(pw), pw))

        # Known passwords as-is
        for pw in KNOWN_PASSWORDS:
            _add(pw)

        # LLM candidates (if any were generated in a previous run)
        for pw in LLM_CANDIDATES:
            _add(pw)

        # --- Expand each word tier ---
        critical = WORD_TIERS.get("critical", [])
        high = WORD_TIERS.get("high", [])
        medium = WORD_TIERS.get("medium", [])
        low = WORD_TIERS.get("low", [])

        leet_max = (
            10 if leet_pct >= 0.5 else
            5 if leet_pct >= 0.2 else
            2
        )

        # Critical + high words: full expansion
        for word in critical + high:
            for variant in case_variants(word):
                _add(variant)
                for num in nums[:12]:
                    _add(f"{variant}{num}")
                    for sep in wseps:
                        _add(f"{variant}{sep}{num}")
                for sep in wseps:
                    _add(f"{variant}{sep}")

            if leet_pct >= 0.1:
                for leet in leet_variants(word, max_variants=leet_max):
                    _add(leet)
                    for num in nums[:8]:
                        _add(f"{leet}{num}")
                    for sep in wseps[:3]:
                        _add(f"{leet}{sep}")
                        for num in nums[:5]:
                            _add(f"{leet}{sep}{num}")

        # Medium words: lighter expansion
        for word in medium:
            for variant in case_variants(word):
                _add(variant)
                for num in nums[:6]:
                    _add(f"{variant}{num}")
                for sep in pref_seps[:2]:
                    _add(f"{variant}{sep}")

        # Low words: minimal
        for word in low:
            _add(word.capitalize())
            _add(word.lower())
            for num in nums[:3]:
                _add(f"{word.capitalize()}{num}")

        # --- Two-word combos (critical + high only) ---
        combo_words = (critical + high)[:15]
        for w1, w2 in itertools.permutations(combo_words[:10], 2):
            for sep in pref_seps[:3]:
                combo = f"{w1.capitalize()}{sep}{w2.capitalize()}"
                _add(combo)
                for num in nums[:5]:
                    _add(f"{combo}{num}")
            camel = f"{w1.capitalize()}{w2.capitalize()}"
            _add(camel)
            for num in nums[:3]:
                _add(f"{camel}{num}")
            if leet_pct >= 0.2:
                for lv in leet_variants(camel, max_variants=3):
                    _add(lv)

        # --- Glue word combos ---
        if GLUE_WORDS:
            for word in combo_words[:10]:
                for glue in GLUE_WORDS[:6]:
                    base = f"{word.capitalize()}{glue.capitalize()}"
                    _add(base)
                    for num in nums[:4]:
                        _add(f"{base}{num}")
                    for sep in pref_seps[:2]:
                        sg = f"{word.capitalize()}{sep}{glue.capitalize()}"
                        _add(sg)
                        for num in nums[:3]:
                            _add(f"{sg}{num}")

        # --- Email/username based ---
        for email in EMAILS:
            user = email.split("@")[0]
            _add(user)
            for num in nums[:5]:
                _add(f"{user}{num}")

        for uname in USERNAMES:
            for variant in case_variants(uname):
                _add(variant)
                for num in nums[:5]:
                    _add(f"{variant}{num}")

        scored.sort(key=lambda x: x[0], reverse=True)
        return scored
''')


# ============================================================================
# HELPER FUNCTIONS — shared by generate_script and generate_script_llm
# ============================================================================

def _classify_words_into_tiers(
    profile: TargetProfile,
    analysis: PatternAnalysis,
) -> dict[str, list[str]]:
    """Classify profile words into priority tiers for generation."""
    critical: list[str] = []
    high: list[str] = []
    medium: list[str] = []
    low: list[str] = []
    seen: set[str] = set()

    skip_words = {"none", "n/a", "no", "unknown", "fake"}

    def _add(word: str, tier: list[str]) -> None:
        if not word or " " in word:
            return
        if word.replace(".", "").isdigit():
            return
        key = word.lower()
        if key in seen or key in skip_words:
            return
        seen.add(key)
        tier.append(word)

    from xora.password_profiler import deleet_to_words
    password_word_counts: Counter = Counter()
    for pw in profile.known_passwords:
        pw_words_unique = {w.lower() for w in deleet_to_words(pw) if len(w) >= 3}
        for w in pw_words_unique:
            password_word_counts[w] += 1

    for val in [profile.first_name, profile.last_name]:
        if val:
            for part in val.split():
                _add(part, high)
    if profile.partner_name:
        for part in profile.partner_name.split():
            _add(part, critical)
    for pet in profile.pet_names:
        _add(pet, critical)
    for nick in profile.nicknames:
        _add(nick, critical)
    for child in profile.children_names:
        if child and child.lower() not in ("none", "n/a", "no", "unknown"):
            _add(child.split()[0], critical)

    for item in analysis.inferred_data:
        word = item.get("word", "")
        conf = item.get("confidence", 0)
        if not word or " " in word:
            continue
        if conf >= 0.7:
            _add(word, critical)
        elif conf >= 0.5:
            _add(word, high)
        elif conf >= 0.3:
            _add(word, medium)
        else:
            _add(word, low)

    for corr in analysis.correlation_data:
        for sw in corr.get("suggested_words", []):
            if sw and " " not in sw:
                _add(sw, high)

    interest_set = {w.lower() for w in (profile.interests + profile.companies)}
    all_base = profile.all_base_words()
    for word in all_base:
        if word.lower() in seen:
            continue
        pw_count = password_word_counts.get(word.lower(), 0)
        if pw_count >= 2:
            _add(word, critical)
        elif pw_count == 1:
            _add(word, high)
        elif word.lower() in interest_set:
            _add(word, high)
        else:
            _add(word, low)

    return {
        "critical": critical,
        "high": high,
        "medium": medium,
        "low": low,
    }


def _extract_leet_map(
    known_passwords: list[str],
    preferred_seps: list[str] | None = None,
) -> dict[str, list[str]]:
    """Build a leet map from the target's actual substitution patterns."""
    sep_set = set(preferred_seps or [])

    leet_to_alpha = {
        "0": "o", "3": "e", "4": "a", "5": "s", "7": "t",
        "8": "b", "9": "g", "@": "a", "$": "s", "!": "i",
        "1": "i",
    }
    sub_counts: dict[str, dict[str, int]] = {}

    for pw in known_passwords:
        chars = list(pw)
        n = len(chars)
        for idx, c in enumerate(chars):
            if c not in leet_to_alpha:
                continue
            has_alpha_neighbor = False
            if idx > 0 and chars[idx - 1].isalpha():
                has_alpha_neighbor = True
            if idx < n - 1 and chars[idx + 1].isalpha():
                has_alpha_neighbor = True
            if not has_alpha_neighbor:
                continue
            alpha = leet_to_alpha[c]
            if alpha not in sub_counts:
                sub_counts[alpha] = {}
            sub_counts[alpha][c] = sub_counts[alpha].get(c, 0) + 1

    if not sub_counts:
        return {
            "a": ["4", "@"], "e": ["3"], "i": ["1", "!"],
            "o": ["0"], "s": ["5", "$"], "t": ["7"],
            "l": ["1"], "b": ["8"], "g": ["9"],
        }

    result: dict[str, list[str]] = {}
    for alpha, subs in sub_counts.items():
        def _sort_key(s: str) -> tuple[int, int]:
            is_sep = 1 if s in sep_set else 0
            return (is_sep, -subs[s])
        ordered = sorted(subs.keys(), key=_sort_key)
        result[alpha] = ordered

    return result


def _extract_separator_fingerprint(
    analysis: PatternAnalysis,
) -> tuple[list[str], list[str]]:
    """Extract preferred and rare separators from correlation data."""
    preferred: list[str] = []
    rare: list[str] = []

    for corr in analysis.correlation_data:
        if corr.get("pattern_name") != "separator_fingerprint":
            continue
        evidence = corr.get("evidence", [])
        for entry in evidence:
            m = re.match(r"'(.+?)' used (\d+)x \((\d+)%\)", entry)
            if m:
                char = m.group(1)
                pct = int(m.group(3))
                if pct >= 15:
                    preferred.append(char)
                else:
                    rare.append(char)
        if preferred:
            return preferred, rare

    if analysis.preferred_specials:
        return analysis.preferred_specials[:4], analysis.preferred_specials[4:]

    return ["!", "_", "#"], ["&"]


def _enrich_numbers(
    profile: TargetProfile,
    analysis: PatternAnalysis,
) -> list[str]:
    """Merge profile numbers with inferred birthday patterns, graduation years,
    and correlation-suggested years.
    """
    nums = list(profile.all_numbers())
    seen = set(nums)

    for item in analysis.inferred_data:
        rule = item.get("rule", "")
        word = item.get("word", "")
        if rule in ("birthday_pattern", "graduation_year") and word:
            if word not in seen and word.replace(".", "").isdigit():
                seen.add(word)
                nums.append(word)

    for corr in analysis.correlation_data:
        for sw in corr.get("suggested_words", []):
            if sw and sw.isdigit() and sw not in seen:
                seen.add(sw)
                nums.append(sw)

    return nums


def _build_data_constants(
    profile: TargetProfile,
    analysis: PatternAnalysis,
    *,
    target_name: str,
    llm_model: str = "none",
    generation_mode: str = "fallback",
    min_length: int = 8,
    max_length: int = 64,
    require_upper: bool = False,
    require_lower: bool = False,
    require_digit: bool = False,
    require_special: bool = False,
    custom_specials: str | None = None,
) -> dict:
    """Build the data dictionary used to fill the script skeleton."""
    tiers = _classify_words_into_tiers(profile, analysis)
    numbers = _enrich_numbers(profile, analysis)
    preferred_seps, rare_seps = _extract_separator_fingerprint(analysis)
    if custom_specials:
        preferred_seps = list(custom_specials)
        rare_seps = []

    cat_weights = {}
    if analysis.password_profile:
        cat_weights = analysis.password_profile.get("priority_weights", {})
        if not cat_weights:
            cat_weights = analysis.password_profile.get("category_distribution", {})

    sp = analysis.strength_profile or {}
    sem = analysis.semantic_analysis or {}
    glue_words = sem.get("glue_words", [])
    semantic_templates = sem.get("semantic_templates", [])
    role_vocabulary = sem.get("role_vocabulary", {})
    leet_map = _extract_leet_map(profile.known_passwords, preferred_seps)

    return {
        "target_name": target_name,
        "version": "0.2.0",
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "script_name": "generate_passwords.py",
        "generation_mode": generation_mode,
        "word_tiers": json.dumps(tiers, indent=4),
        "numbers": json.dumps(numbers, indent=4),
        "known_passwords": json.dumps(profile.known_passwords, indent=4),
        "emails": json.dumps(profile.emails, indent=4),
        "usernames": json.dumps(profile.usernames, indent=4),
        "cap_style": analysis.capitalization_style,
        "avg_length": round(analysis.avg_length, 1),
        "pattern_templates": json.dumps(analysis.unique_templates, indent=4),
        "category_weights": json.dumps(cat_weights, indent=4),
        "preferred_separators": json.dumps(preferred_seps),
        "rare_separators": json.dumps(rare_seps),
        "strength_tier": sp.get("tier", "unknown"),
        "strength_avg_score": round(sp.get("avg_score", 0.5), 2),
        "strength_reuse_ratio": round(sp.get("reuse_ratio", 0.0), 2),
        "strength_leet_pct": round(sp.get("leet_usage_pct", 0.0), 2),
        "strength_always_special": sp.get("always_has_special", False),
        "strength_always_digit": sp.get("always_has_digit", False),
        "strength_always_upper": sp.get("always_has_upper", False),
        "llm_candidates": json.dumps([], indent=4),
        "llm_model": llm_model,
        "leet_map": json.dumps(leet_map, indent=4),
        "glue_words": json.dumps(glue_words, indent=4),
        "semantic_templates": json.dumps(semantic_templates, indent=4),
        "role_vocabulary": json.dumps(role_vocabulary, indent=4),
        "min_length": min_length,
        "max_length": max_length,
        "require_upper": require_upper,
        "require_lower": require_lower,
        "require_digit": require_digit,
        "require_special": require_special,
    }


# ============================================================================
# SCRIPT GENERATION — two paths: LLM-driven or fallback
# ============================================================================

def generate_script(
    profile: TargetProfile,
    analysis: PatternAnalysis,
    *,
    target_name: str,
    llm_model: str = "none",
    min_length: int = 8,
    max_length: int = 64,
    require_upper: bool = False,
    require_lower: bool = False,
    require_digit: bool = False,
    require_special: bool = False,
    custom_specials: str | None = None,
) -> str:
    """Generate script using the fallback (no-LLM) engine."""
    data = _build_data_constants(
        profile, analysis,
        target_name=target_name,
        llm_model=llm_model,
        generation_mode="fallback (no LLM)",
        min_length=min_length,
        max_length=max_length,
        require_upper=require_upper,
        require_lower=require_lower,
        require_digit=require_digit,
        require_special=require_special,
        custom_specials=custom_specials,
    )
    data["generation_engine"] = _FALLBACK_ENGINE
    return _SCRIPT_SKELETON.format(**data)


def generate_script_llm(
    profile: TargetProfile,
    analysis: PatternAnalysis,
    *,
    target_name: str,
    llm_model: str = "none",
    custom_code: str,
    min_length: int = 8,
    max_length: int = 64,
    require_upper: bool = False,
    require_lower: bool = False,
    require_digit: bool = False,
    require_special: bool = False,
    custom_specials: str | None = None,
) -> str:
    """Generate script using LLM-written generation code."""
    data = _build_data_constants(
        profile, analysis,
        target_name=target_name,
        llm_model=llm_model,
        generation_mode=f"LLM-driven ({llm_model})",
        min_length=min_length,
        max_length=max_length,
        require_upper=require_upper,
        require_lower=require_lower,
        require_digit=require_digit,
        require_special=require_special,
        custom_specials=custom_specials,
    )
    data["generation_engine"] = custom_code
    return _SCRIPT_SKELETON.format(**data)


def build_intelligence_summary(
    profile: TargetProfile,
    analysis: PatternAnalysis,
) -> dict:
    """Build a compact intelligence summary for LLM code generation.

    This is the data package the LLM receives when writing custom
    generate_all() code — it tells the LLM everything about the target's
    password psychology.
    """
    tiers = _classify_words_into_tiers(profile, analysis)
    preferred_seps, rare_seps = _extract_separator_fingerprint(analysis)
    sp = analysis.strength_profile or {}
    sem = analysis.semantic_analysis or {}

    return {
        "word_tiers": tiers,
        "numbers": _enrich_numbers(profile, analysis),
        "known_passwords": profile.known_passwords,
        "cap_style": analysis.capitalization_style,
        "avg_length": round(analysis.avg_length, 1),
        "pattern_templates": analysis.unique_templates,
        "preferred_separators": preferred_seps,
        "rare_separators": rare_seps,
        "strength_tier": sp.get("tier", "unknown"),
        "leet_usage_pct": sp.get("leet_usage_pct", 0.0),
        "avg_score": sp.get("avg_score", 0.5),
        "glue_words": sem.get("glue_words", []),
        "semantic_templates": sem.get("semantic_templates", []),
        "role_vocabulary": sem.get("role_vocabulary", {}),
        "leet_map": _extract_leet_map(profile.known_passwords, preferred_seps),
        "category_weights": (
            analysis.password_profile.get("priority_weights", {})
            if analysis.password_profile else {}
        ),
    }


# ============================================================================
# WRITE TARGET FOLDER — creates all artifacts
# ============================================================================

def write_target_folder(
    target_dir: Path,
    profile: TargetProfile,
    analysis: PatternAnalysis,
    *,
    target_name: str,
    llm_model: str = "none",
    custom_code: str | None = None,
    raw_text: str = "",
    min_length: int = 8,
    max_length: int = 64,
    require_upper: bool = False,
    require_lower: bool = False,
    require_digit: bool = False,
    require_special: bool = False,
    custom_specials: str | None = None,
    cached_steps: list[str] | None = None,
) -> Path:
    """Create the target folder with all artifacts."""
    target_dir.mkdir(parents=True, exist_ok=True)

    # Raw profile
    raw_text_final = raw_text or profile.raw_text
    raw_path = target_dir / "profile.raw"
    raw_path.write_text(raw_text_final, encoding="utf-8")

    # Cache metadata for subsequent `xora analyze` runs
    profile_hash = hashlib.sha256(raw_text_final.encode()).hexdigest()
    all_steps = cached_steps or [
        "parse", "categorize", "semantics", "inference",
        "correlations", "curate", "custom_code",
    ]
    cache_meta = {
        "profile_hash": profile_hash,
        "llm_model": llm_model,
        "timestamp": datetime.now().isoformat(),
        "cached_steps": all_steps,
    }
    cache_path = target_dir / "cache_meta.json"
    cache_path.write_text(json.dumps(cache_meta, indent=2), encoding="utf-8")

    # Parsed profile JSON
    parsed_path = target_dir / "profile.parsed.json"
    parsed_path.write_text(
        json.dumps(profile.to_dict(), indent=2), encoding="utf-8"
    )

    # Pattern analysis
    analysis_path = target_dir / "analysis.json"
    analysis_path.write_text(
        json.dumps(analysis.to_dict(), indent=2), encoding="utf-8"
    )

    # Build tier info for the report
    tiers = _classify_words_into_tiers(profile, analysis)

    # Human-readable analysis report
    report_lines = _build_report(
        profile, analysis, tiers, target_name, llm_model,
    )
    report_path = target_dir / "analysis.md"
    report_path.write_text("\n".join(report_lines) + "\n", encoding="utf-8")

    # The main artifact: standalone generation script
    if custom_code:
        script = generate_script_llm(
            profile, analysis,
            target_name=target_name,
            llm_model=llm_model,
            custom_code=custom_code,
            min_length=min_length,
            max_length=max_length,
            require_upper=require_upper,
            require_lower=require_lower,
            require_digit=require_digit,
            require_special=require_special,
            custom_specials=custom_specials,
        )
    else:
        script = generate_script(
            profile, analysis,
            target_name=target_name,
            llm_model=llm_model,
            min_length=min_length,
            max_length=max_length,
            require_upper=require_upper,
            require_lower=require_lower,
            require_digit=require_digit,
            require_special=require_special,
            custom_specials=custom_specials,
        )
    script_path = target_dir / "generate_passwords.py"
    script_path.write_text(script, encoding="utf-8")
    script_path.chmod(0o755)

    return target_dir


def _build_report(
    profile: TargetProfile,
    analysis: PatternAnalysis,
    tiers: dict,
    target_name: str,
    llm_model: str,
) -> list[str]:
    """Build the human-readable analysis.md report."""
    report_lines = [
        f"# xora Analysis Report: {target_name}",
        f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        "",
        "## Word Tiers",
    ]
    for tier_name in ["critical", "high", "medium", "low"]:
        words = tiers.get(tier_name, [])
        report_lines.append(f"  **{tier_name}** ({len(words)}): {', '.join(words) or '(none)'}")

    report_lines.extend([
        "",
        "## Numbers",
        ", ".join(_enrich_numbers(profile, analysis)) or "(none extracted)",
        "",
        "## Known Passwords",
    ])
    if profile.known_passwords:
        for pw in profile.known_passwords:
            report_lines.append(f"  - `{pw}`")
    else:
        report_lines.append("  (none)")

    report_lines.extend([
        "",
        "## Pattern Analysis",
        f"- Capitalization style: {analysis.capitalization_style}",
        f"- Number style: {analysis.number_style}",
        f"- Average password length: {analysis.avg_length:.1f}",
    ])
    preferred_seps, rare_seps = _extract_separator_fingerprint(analysis)
    report_lines.append(f"- Preferred separators: {', '.join(repr(s) for s in preferred_seps)}")
    if rare_seps:
        report_lines.append(f"- Rare separators: {', '.join(repr(s) for s in rare_seps)}")

    report_lines.extend(["", "### Detected Templates"])
    for pat in analysis.patterns:
        report_lines.append(f"  - `{pat.source}` → `{pat.template}` ({pat.description})")
    if not analysis.patterns:
        report_lines.append("  (no known passwords to analyze)")

    pw_prof = analysis.password_profile
    if pw_prof and pw_prof.get("categorized_passwords"):
        report_lines.extend(["", "## Password Psychology Profile"])
        cat_dist = pw_prof.get("category_distribution", {})
        if cat_dist:
            report_lines.extend(["", "### Category Distribution"])
            for cat, pct in cat_dist.items():
                bar = "█" * int(pct * 40)
                report_lines.append(f"  {cat:<16} {pct:>5.1%}  {bar}")
        top = pw_prof.get("top_categories", [])
        if top:
            report_lines.extend(["", f"**Primary themes:** {', '.join(top[:5])}"])
        report_lines.extend(["", "### Decoded Passwords"])
        for cp in pw_prof["categorized_passwords"]:
            conf = cp.get("confidence", 0)
            reasoning = cp.get("reasoning", "")
            reason_str = f" — {reasoning}" if reasoning else ""
            report_lines.append(
                f"  - `{cp['original']}` → `{cp['decoded']}` "
                f"[{cp['category']}] ({conf:.0%}){reason_str}"
            )
        prio = pw_prof.get("priority_weights")
        if prio:
            report_lines.extend(["", "### Generation Priority Weights"])
            for cat, w in prio.items():
                report_lines.append(f"  {cat:<16} {w:.1%}")

    sp = analysis.strength_profile
    if sp:
        report_lines.extend(["", "## Password Strength Assessment", ""])
        report_lines.append(
            f"**Overall: {sp.get('tier', 'unknown').upper()}** "
            f"(avg score: {sp.get('avg_score', 0):.2f}/1.00)"
        )
        report_lines.extend([
            "",
            "| Metric | Value |",
            "|--------|-------|",
            f"| Avg length | {sp.get('avg_length', 0):.1f} |",
            f"| Length range | {sp.get('min_length', 0)}-{sp.get('max_length', 0)} |",
            f"| Avg char classes | {sp.get('avg_char_classes', 0):.1f}/4 |",
            f"| Avg entropy | {sp.get('avg_entropy', 0):.0f} bits |",
            f"| Leet speak usage | {sp.get('leet_usage_pct', 0):.0%} |",
            f"| Word reuse | {sp.get('reuse_ratio', 0):.0%} |",
        ])

        indiv = sp.get("individual", [])
        if indiv:
            report_lines.extend(["", "### Per-Password Breakdown", ""])
            report_lines.append("| Password | Score | Tier | Length | Classes | Entropy |")
            report_lines.append("|----------|-------|------|--------|---------|---------|")
            for s in indiv:
                report_lines.append(
                    f"| `{s['password']}` | {s['score']:.2f} | {s['tier']} "
                    f"| {s['length']} | {s['char_classes']}/4 "
                    f"| {s['entropy_bits']:.0f} |"
                )

    if analysis.inferred_data:
        report_lines.extend(["", "## Inferred Intelligence",
                             "", "Words derived from profile data by reasoning about context:"])
        rules_seen: dict[str, list] = {}
        for item in analysis.inferred_data:
            rule = item.get("rule", "unknown")
            rules_seen.setdefault(rule, []).append(item)
        for rule, items in rules_seen.items():
            report_lines.append(f"\n### {rule.replace('_', ' ').title()}")
            for item in items:
                conf = item.get("confidence", 0)
                report_lines.append(
                    f"  - **{item.get('word', '')}** ({conf:.0%}) "
                    f"— {item.get('reasoning', '')}"
                )

    if analysis.correlation_data:
        report_lines.extend(["", "## Correlation Insights",
                             "", "Hidden connections between passwords and profile data:"])
        for corr in analysis.correlation_data:
            name = corr.get("pattern_name", "")
            report_lines.append(f"\n### {name.replace('_', ' ').title()}")
            report_lines.append(f"**Insight:** {corr.get('insight', '')}")
            conf = corr.get("confidence", 0)
            report_lines.append(f"**Confidence:** {conf:.0%}")
            evidence = corr.get("evidence", [])
            if evidence:
                report_lines.append("**Evidence:**")
                for e in evidence:
                    report_lines.append(f"  - {e}")
            suggested = corr.get("suggested_words", [])
            if suggested:
                report_lines.append(
                    f"**Suggested words:** {', '.join(suggested)}"
                )

    sem = analysis.semantic_analysis or {}
    sem_passwords = sem.get("passwords", [])
    sem_glue = sem.get("glue_words", [])
    sem_templates = sem.get("semantic_templates", [])
    sem_roles = sem.get("role_vocabulary", {})
    if sem_passwords:
        report_lines.extend([
            "",
            "## Semantic Decomposition",
            "",
            "Each password broken down by *meaning*, not just structure:",
            "",
        ])
        for sp_item in sem_passwords[:15]:
            comps = sp_item.get("components", [])
            comp_strs = [
                f"**{c['role']}**: `{c['value']}`" for c in comps
            ]
            report_lines.append(
                f"- `{sp_item['original']}` → {' + '.join(comp_strs)}"
            )
            report_lines.append(
                f"  - Template: `{sp_item.get('semantic_template', '')}`"
                f" | Category: {sp_item.get('category', 'unknown')}"
            )
        if len(sem_passwords) > 15:
            report_lines.append(f"- ... and {len(sem_passwords) - 15} more")
        if sem_glue:
            report_lines.extend([
                "", "### Glue Words", "",
                f"**Detected:** {', '.join(f'`{g}`' for g in sem_glue)}",
            ])
        if sem_templates:
            report_lines.extend(["", "### Semantic Templates", ""])
            for tmpl in sem_templates[:10]:
                report_lines.append(f"- `{tmpl}`")
        if sem_roles:
            report_lines.extend(["", "### Role Vocabulary", ""])
            for role, words in sem_roles.items():
                report_lines.append(
                    f"- **{role}**: {', '.join(f'`{w}`' for w in words[:8])}"
                    f"{'...' if len(words) > 8 else ''}"
                )

    return report_lines
