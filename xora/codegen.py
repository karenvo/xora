"""Generate standalone password generation scripts for a target."""

from __future__ import annotations

import hashlib
import json
import re
import textwrap
from collections import Counter
from datetime import datetime
from pathlib import Path

from xora.pattern_analyzer import DerivationChain, PatternAnalysis
from xora.profile_parser import TargetProfile

# ============================================================================
# SCRIPT SKELETON — boilerplate shared by both LLM and fallback scripts.
# Contains: imports, data constants, utility functions (leet_variants,
# case_variants, policy, scoring, output formatting), argparse, main().
#
# The generation engine (generate_all + helpers) is injected separately
# via {generation_engine}.
# ============================================================================

_SCRIPT_SKELETON = textwrap.dedent('''\
    #!/usr/bin/env python3
    """Password candidate generator for target: {target_name}
    Generated by xora v{version} on {timestamp}
    Mode: {generation_mode}

    Usage:
        python3 {script_name}                          # dump all to stdout
        python3 {script_name} --ranked                 # sort by likelihood
        python3 {script_name} --limit 500              # cap output
        python3 {script_name} --min-length 10          # override policy
        python3 {script_name} --format hydra           # user:pass pairs
        python3 {script_name} -o wordlist.txt          # write to file

    Pipe directly into attack tools:
        python3 {script_name} | hydra -l user -P /dev/stdin ssh://target
        python3 {script_name} | hashcat -m 1000 hashes.txt
    """

    import argparse
    import itertools
    import random
    import re
    import sys
    from typing import Iterator

    # ========================================================================
    # PROFILE DATA — tiered by password likelihood
    # ========================================================================

    WORD_TIERS = {word_tiers}

    NUMBERS = {numbers}

    KNOWN_PASSWORDS = {known_passwords}

    EMAILS = {emails}

    USERNAMES = {usernames}

    # ========================================================================
    # DETECTED PATTERNS — from known password analysis
    # ========================================================================

    CAP_STYLE = "{cap_style}"
    # Observed capitalization pattern distribution across known passwords.
    # Keys: all_lower | all_upper | first_only | camel | alternating | mixed
    # Values: count of words exhibiting that pattern.
    CAP_PATTERNS = {cap_patterns}
    AVG_LENGTH = {avg_length}
    PATTERN_TEMPLATES = {pattern_templates}

    # ========================================================================
    # PASSWORD PSYCHOLOGY PROFILE
    # ========================================================================

    CATEGORY_WEIGHTS = {category_weights}

    # ========================================================================
    # SEPARATOR FINGERPRINT
    # ========================================================================

    PREFERRED_SEPARATORS = {preferred_separators}
    RARE_SEPARATORS = {rare_separators}

    # ========================================================================
    # PASSWORD STRENGTH PROFILE
    # ========================================================================

    STRENGTH_TIER = "{strength_tier}"
    STRENGTH_AVG_SCORE = {strength_avg_score}
    STRENGTH_REUSE_RATIO = {strength_reuse_ratio}
    STRENGTH_LEET_PCT = {strength_leet_pct}
    STRENGTH_ALWAYS_SPECIAL = {strength_always_special}
    STRENGTH_ALWAYS_DIGIT = {strength_always_digit}
    STRENGTH_ALWAYS_UPPER = {strength_always_upper}

    # ========================================================================
    # LLM-GENERATED CANDIDATES — psychologically targeted by AI
    # Model: {llm_model} | Generated: {timestamp}
    # ========================================================================

    LLM_CANDIDATES = {llm_candidates}

    # ========================================================================
    # GLUE WORDS & SEMANTIC DATA
    # ========================================================================

    GLUE_WORDS = {glue_words}
    SEMANTIC_TEMPLATES = {semantic_templates}
    ROLE_VOCABULARY = {role_vocabulary}

    # ========================================================================
    # DERIVATION CHAINS — detected progression sequences in known passwords
    # Each chain has: base_word, chain_type, members, next_likely candidates
    # Use these high-confidence candidates early in the generation pass.
    # ========================================================================

    DERIVATION_CHAINS = {derivation_chains}

    # ========================================================================
    # PASSWORD POLICY
    # ========================================================================

    DEFAULT_POLICY = {{
        "min_length": {min_length},
        "max_length": {max_length},
        "require_upper": {require_upper},
        "require_lower": {require_lower},
        "require_digit": {require_digit},
        "require_special": {require_special},
    }}

    # ========================================================================
    # LEET SPEAK MAP — built from target's actual substitution patterns
    # ========================================================================

    LEET_MAP = {leet_map}

    # When True, leet_variants() produces every possible substitution
    # combination instead of sampling. Output grows exponentially — only
    # use when the word pool is small or you want maximum coverage.
    LEET_EXHAUSTIVE = {leet_exhaustive}

    # ========================================================================
    # UTILITY FUNCTIONS
    # ========================================================================

    def _all_words() -> list[str]:
        """Flatten WORD_TIERS into a single deduped list, critical first."""
        seen = set()
        words = []
        for tier in ["critical", "high", "medium", "low"]:
            for w in WORD_TIERS.get(tier, []):
                if w not in seen:
                    seen.add(w)
                    words.append(w)
        return words


    def _apply_cap_pattern(word: str, pattern: str) -> str:
        """Apply a named capitalization pattern to a base word."""
        if pattern == "all_lower":
            return word.lower()
        if pattern == "all_upper":
            return word.upper()
        if pattern == "first_only":
            return word[0].upper() + word[1:].lower() if word else word
        if pattern == "camel":
            # For a single word, CamelCase = capitalize; compound words produced
            # by the combinator join two already-capitalized halves (e.g. MotleyCrue).
            return word[0].upper() + word[1:].lower() if word else word
        if pattern == "alternating":
            return "".join(
                c.upper() if i % 2 == 0 else c.lower() for i, c in enumerate(word)
            )
        # mixed / fallback: return as-is
        return word


    def case_variants(word: str) -> list[str]:
        """Generate capitalization variants weighted by observed cap patterns.

        Most-frequent patterns in CAP_PATTERNS are tried first. A floor of
        three variants (lower, capitalize, upper) is always included so
        generation stays broad even for small password sets.
        """
        seen: set[str] = set()
        results: list[str] = []

        total = sum(CAP_PATTERNS.values()) if CAP_PATTERNS else 0

        if total > 0:
            for pattern, count in sorted(CAP_PATTERNS.items(), key=lambda kv: -kv[1]):
                if count / total < 0.05 and len(results) >= 4:
                    break
                v = _apply_cap_pattern(word, pattern)
                if v not in seen:
                    seen.add(v)
                    results.append(v)

        # Always include basic fallbacks in priority order from legacy CAP_STYLE
        for fallback in (
            word.lower(),
            word[0].upper() + word[1:].lower() if word else word,
            word.upper(),
        ):
            if fallback not in seen:
                seen.add(fallback)
                results.append(fallback)

        return results


    def leet_variants(word: str, max_variants: int = 5) -> list[str]:
        """Apply leet speak substitutions matching the target's style.

        In exhaustive mode (LEET_EXHAUSTIVE=True) every combination of
        substitution sites is generated via itertools.product — use for
        maximum coverage when the word pool is small.

        In sampled mode each substitution site is applied independently,
        capped at max_variants * 3 — fast for large word pools.
        """
        lower = word.lower()

        # Collect (position_in_word, original_char, leet_char) for each site
        sites: list[tuple[int, str, str]] = []
        for leet_char, replacements in LEET_MAP.items():
            primary = replacements[0]
            idx = 0
            while True:
                pos = lower.find(leet_char, idx)
                if pos == -1:
                    break
                sites.append((pos, lower[pos], primary))
                idx = pos + 1

        if not sites:
            return [word]

        if LEET_EXHAUSTIVE:
            # Build all 2^n combos: each site is either substituted or not
            results = set()
            for combo in itertools.product(*[
                (False, True) for _ in sites
            ]):
                chars = list(word)
                for apply, (pos, orig, leet) in zip(combo, sites):
                    if apply:
                        # Preserve original case: if original was upper, leet is upper
                        chars[pos] = leet.upper() if word[pos].isupper() else leet
                results.add("".join(chars))
            return sorted(results)
        else:
            # Sampled: apply one site at a time, cap results
            results = [word]
            for _pos, orig, primary in sites:
                new_results = []
                for current in results[:max_variants]:
                    new_results.append(current.replace(orig, primary, 1))
                    new_results.append(current.replace(orig.upper(), primary, 1))
                results.extend(new_results)
            return list(set(results))[:max_variants * 3]


    def _weighted_seps(seps: list[str], total: int = 6) -> list[str]:
        """Expand separator list weighted by preference order."""
        if not seps:
            return ["!", "_", "#"]
        if len(seps) == 1:
            return seps
        result: list[str] = []
        remaining = total
        for i, s in enumerate(seps):
            if i == 0:
                count = max(1, (total + 1) // 2)
            elif i == 1:
                count = max(1, (remaining + 1) // 2)
            else:
                count = max(1, remaining // max(1, len(seps) - i))
            count = min(count, remaining)
            result.extend([s] * count)
            remaining -= count
            if remaining <= 0:
                break
        return result


    def number_suffixes() -> list[str]:
        """Number suffixes — profile numbers first, then common ones."""
        suffixes = list(NUMBERS)
        for n in NUMBERS:
            if len(n) == 4:
                short = n[2:]
                if short not in suffixes:
                    suffixes.append(short)
        for n in ["1", "12", "123", "1234", "01", "007", "69", "420"]:
            if n not in suffixes:
                suffixes.append(n)
        current_year = 2026
        for y in range(current_year - 5, current_year + 2):
            for s in [str(y), str(y)[2:]]:
                if s not in suffixes:
                    suffixes.append(s)
        return suffixes


    # Leet chars that real password policies count as "special characters"
    # (non-alphanumeric) — @, $, !, | are specials; 0, 3, 1, 4 are digits.
    _LEET_SPECIALS = frozenset("@$!|+(€¡")
    _LEET_DIGITS   = frozenset("01345789")  # leet digit substitutes

    def passes_policy(password: str, policy: dict) -> bool:
        """Check if a password meets the configured policy.

        Leet substitutions are correctly accounted for:
          - @, $, !, | etc.  count as special characters
          - 0, 3, 1, 4 etc.  count as digits (they ARE digits to Python too)
          - No leet sub       counts as uppercase (must have a real A-Z)
        """
        if len(password) < policy["min_length"]:
            return False
        if len(password) > policy["max_length"]:
            return False
        if policy["require_upper"] and not any(c.isupper() for c in password):
            return False
        if policy["require_lower"] and not any(c.islower() for c in password):
            return False
        if policy["require_digit"]:
            # c.isdigit() already covers leet digits (0,1,3,4,5,7,8,9)
            if not any(c.isdigit() for c in password):
                return False
        if policy["require_special"]:
            # A password satisfies this if it has any non-alphanumeric char
            # (which includes leet specials like @, $, !) OR any leet-special
            # that Python counts as alphanumeric but is used as punctuation.
            has_special = any(
                not c.isalnum() or c in _LEET_SPECIALS
                for c in password
            )
            if not has_special:
                return False
        return True


    def _segment(password: str) -> str:
        """Structural fingerprint like 'WSWN'."""
        out = []
        prev = ""
        for c in password:
            if c.isalpha():
                kind = "W"
            elif c.isdigit():
                kind = "N"
            else:
                kind = "S"
            if kind != prev:
                out.append(kind)
                prev = kind
        return "".join(out)


    _TEMPLATE_FINGERPRINTS: list[str] = []
    _LLM_LOWER: set[str] = {{p.lower() for p in LLM_CANDIDATES}}


    def _init_fingerprints() -> None:
        if _TEMPLATE_FINGERPRINTS:
            return
        for tmpl in PATTERN_TEMPLATES:
            fp = ""
            for m in re.finditer(r"\\{{(\\w+)\\}}", tmpl):
                kind = m.group(1)
                if kind.startswith("word"):
                    fp += "W"
                elif kind.startswith(("year", "num")):
                    fp += "N"
                elif kind in ("special",):
                    fp += "S"
            if fp:
                _TEMPLATE_FINGERPRINTS.append(fp)


    def score_candidate(password: str) -> float:
        """Score a password by structural match, word tiers, separators, etc."""
        score = 0.3
        _init_fingerprints()

        pw_fp = _segment(password)
        best_struct = 0.0
        for tmpl_fp in _TEMPLATE_FINGERPRINTS:
            if pw_fp == tmpl_fp:
                best_struct = 0.3
                break
            elif tmpl_fp.startswith(pw_fp) or pw_fp.startswith(tmpl_fp):
                best_struct = max(best_struct, 0.15)
        score += best_struct

        lower_pw = password.lower()
        tier_bonuses = {{"critical": 0.25, "high": 0.15, "medium": 0.08, "low": 0.03}}
        best_tier_bonus = 0.0
        for tier_name, bonus in tier_bonuses.items():
            for w in WORD_TIERS.get(tier_name, []):
                if w.lower() in lower_pw:
                    best_tier_bonus = max(best_tier_bonus, bonus)
                    break
            if best_tier_bonus >= 0.25:
                break
        score += best_tier_bonus

        specials_in_pw = [c for c in password if not c.isalnum()]
        if specials_in_pw:
            pref_set = set(PREFERRED_SEPARATORS)
            rare_set = set(RARE_SEPARATORS)
            all_preferred = all(c in pref_set or c in rare_set for c in specials_in_pw)
            if all_preferred:
                if all(c in pref_set for c in specials_in_pw):
                    score += 0.10
                else:
                    score += 0.05
            else:
                score -= 0.05

        if AVG_LENGTH > 0:
            diff = abs(len(password) - AVG_LENGTH)
            if diff <= 2:
                score += 0.10
            elif diff <= 5:
                score += 0.05

        if lower_pw in _LLM_LOWER:
            score += 0.15

        if GLUE_WORDS:
            for gw in GLUE_WORDS:
                if gw.lower() in lower_pw:
                    score += 0.08
                    break

        if len(specials_in_pw) > 2:
            score -= 0.1

        # Cap-pattern bonus: reward passwords whose words follow the dominant
        # observed capitalization pattern (up to +0.10).
        if CAP_PATTERNS:
            dominant_pattern = max(CAP_PATTERNS, key=CAP_PATTERNS.get)
            dominant_count = CAP_PATTERNS[dominant_pattern]
            total_cap = sum(CAP_PATTERNS.values())
            dominant_weight = dominant_count / total_cap if total_cap else 0
            # Only award the bonus when the pattern is genuinely dominant (>40%)
            if dominant_weight >= 0.40:
                import re as _re
                alpha_words = _re.findall(r"[A-Za-z]+", password)
                if alpha_words:
                    # Check the longest alpha word in the password
                    sample = max(alpha_words, key=len)
                    alpha_chars = [c for c in sample if c.isalpha()]
                    upper_c = sum(1 for c in alpha_chars if c.isupper())
                    total_a = len(alpha_chars)
                    if total_a:
                        pw_pattern = (
                            "all_lower" if upper_c == 0 else
                            "all_upper" if upper_c == total_a else
                            "first_only" if upper_c == 1 and alpha_chars[0].isupper() else
                            "camel" if any(
                                alpha_chars[i].islower() and alpha_chars[i + 1].isupper()
                                for i in range(len(alpha_chars) - 1)
                            ) else "mixed"
                        )
                        if pw_pattern == dominant_pattern:
                            score += round(0.10 * dominant_weight, 3)

        return min(max(score, 0.0), 1.0)


    # ========================================================================
    # GENERATION ENGINE — injected by xora (LLM-written or fallback)
    # ========================================================================

    {generation_engine}


    # ========================================================================
    # OUTPUT & CLI
    # ========================================================================

    def format_output(scored: list[tuple[float, str]], fmt: str,
                      username: str = "") -> Iterator[str]:
        """Format output for different attack tools."""
        if fmt == "hydra":
            user = username or (EMAILS[0].split("@")[0] if EMAILS else "user")
            for _, pw in scored:
                yield f"{{user}}:{{pw}}"
        elif fmt == "credstuff":
            for email in (EMAILS or ["user@example.com"]):
                for _, pw in scored:
                    yield f"{{email}}:{{pw}}"
        elif fmt == "burp":
            seen = set()
            for _, pw in scored:
                if pw not in seen:
                    seen.add(pw)
                    yield pw
        else:
            for _, pw in scored:
                yield pw


    def main() -> None:
        parser = argparse.ArgumentParser(
            description=f"xora password generator for: {target_name}",
            formatter_class=argparse.RawDescriptionHelpFormatter,
        )
        parser.add_argument("--ranked", action="store_true",
                            help="sort by likelihood (most probable first)")
        parser.add_argument("--limit", type=int, default=0,
                            help="max number of passwords to output")
        parser.add_argument("--min-length", type=int, default=None,
                            help="override minimum password length")
        parser.add_argument("--max-length", type=int, default=None,
                            help="override maximum password length")
        parser.add_argument("--require-upper", action="store_true", default=None,
                            help="require uppercase letter")
        parser.add_argument("--no-require-upper", action="store_true",
                            help="don't require uppercase letter")
        parser.add_argument("--require-lower", action="store_true", default=None,
                            help="require lowercase letter")
        parser.add_argument("--no-require-lower", action="store_true",
                            help="don't require lowercase letter")
        parser.add_argument("--require-digit", action="store_true", default=None,
                            help="require digit")
        parser.add_argument("--no-require-digit", action="store_true",
                            help="don't require digit")
        parser.add_argument("--require-special", action="store_true", default=None,
                            help="require special characters")
        parser.add_argument("--no-require-special", action="store_true",
                            help="don't require special characters")
        parser.add_argument("--format", choices=["plain", "hydra", "burp", "credstuff"],
                            default="plain", help="output format (default: plain)")
        parser.add_argument("--username", default="",
                            help="username for hydra/credstuff format")
        parser.add_argument("-o", "--output", default=None,
                            help="write to file instead of stdout")
        args = parser.parse_args()

        policy = dict(DEFAULT_POLICY)
        if args.min_length is not None:
            policy["min_length"] = args.min_length
        if args.max_length is not None:
            policy["max_length"] = args.max_length
        for _fld in ("upper", "lower", "digit", "special"):
            _no = getattr(args, f"no_require_{{_fld}}", False)
            _yes = getattr(args, f"require_{{_fld}}", None)
            if _no:
                policy[f"require_{{_fld}}"] = False
            elif _yes:
                policy[f"require_{{_fld}}"] = True

        total_words = sum(len(v) for v in WORD_TIERS.values())
        print(f"[xora] Target: {target_name}", file=sys.stderr)
        print(f"[xora] Mode: {generation_mode}", file=sys.stderr)
        print(f"[xora] Words: {{total_words}} | LLM candidates: {{len(LLM_CANDIDATES)}}", file=sys.stderr)
        print(f"[xora] Separators: preferred={{PREFERRED_SEPARATORS}} rare={{RARE_SEPARATORS}}",
              file=sys.stderr)
        print(f"[xora] Strength: {{STRENGTH_TIER}} (leet: {{STRENGTH_LEET_PCT:.0%}})",
              file=sys.stderr)

        scored = generate_all(policy)

        print(f"[xora] Candidates generated: {{len(scored):,}}", file=sys.stderr)

        output_lines = list(format_output(scored, args.format, args.username))
        if args.limit:
            output_lines = output_lines[:args.limit]

        if args.output:
            with open(args.output, "w") as f:
                f.write("\\n".join(output_lines) + "\\n")
            print(f"[xora] Written to {{args.output}}", file=sys.stderr)
        else:
            print("\\n".join(output_lines))


    target_name = "{target_name}"

    if __name__ == "__main__":
        main()
''')

# ============================================================================
# FALLBACK ENGINE — used when no LLM is available.
# Simpler combinatorial approach: expand words with case/leet/numbers/seps.
# No blind permutation explosion. Clear that this is basic mode.
# ============================================================================

_FALLBACK_ENGINE = textwrap.dedent('''\
    def generate_all(policy: dict | None = None) -> list[tuple[float, str]]:
        """Generate candidates using basic combinatorial expansion.

        This is the fallback engine (no LLM available). It expands words from
        the tiered word pool with case variants, leet, numbers, and separators.
        For better results, rerun with an LLM: xora analyze --llm ollama
        """
        pol = policy or DEFAULT_POLICY
        nums = number_suffixes()
        pref_seps = PREFERRED_SEPARATORS or ["!", "_", "#"]
        wseps = _weighted_seps(pref_seps, total=6)
        all_words = _all_words()
        leet_pct = STRENGTH_LEET_PCT

        seen: set[str] = set()
        scored: list[tuple[float, str]] = []

        def _add(pw: str) -> None:
            if pw in seen or " " in pw:
                return
            seen.add(pw)
            if passes_policy(pw, pol):
                scored.append((score_candidate(pw), pw))

        # Derivation chain next_likely candidates — highest confidence predictions
        for chain in DERIVATION_CHAINS:
            for pw in chain.get("next_likely", []):
                _add(pw)
                # Also apply case and leet variants for each chain prediction
                for v in case_variants(pw):
                    _add(v)
                for lv in leet_variants(pw):
                    _add(lv)

        # Known passwords as-is
        for pw in KNOWN_PASSWORDS:
            _add(pw)

        # LLM candidates (if any were generated in a previous run)
        for pw in LLM_CANDIDATES:
            _add(pw)

        # --- Expand each word tier ---
        critical = WORD_TIERS.get("critical", [])
        high = WORD_TIERS.get("high", [])
        medium = WORD_TIERS.get("medium", [])
        low = WORD_TIERS.get("low", [])

        leet_max = (
            10 if leet_pct >= 0.5 else
            5 if leet_pct >= 0.2 else
            2
        )

        # Critical + high words: full expansion
        for word in critical + high:
            for variant in case_variants(word):
                _add(variant)
                for num in nums[:12]:
                    _add(f"{variant}{num}")
                    for sep in wseps:
                        _add(f"{variant}{sep}{num}")
                for sep in wseps:
                    _add(f"{variant}{sep}")

            if leet_pct >= 0.1:
                for leet in leet_variants(word, max_variants=leet_max):
                    _add(leet)
                    for num in nums[:8]:
                        _add(f"{leet}{num}")
                    for sep in wseps[:3]:
                        _add(f"{leet}{sep}")
                        for num in nums[:5]:
                            _add(f"{leet}{sep}{num}")

        # Medium words: lighter expansion
        for word in medium:
            for variant in case_variants(word):
                _add(variant)
                for num in nums[:6]:
                    _add(f"{variant}{num}")
                for sep in pref_seps[:2]:
                    _add(f"{variant}{sep}")

        # Low words: minimal
        for word in low:
            _add(word.capitalize())
            _add(word.lower())
            for num in nums[:3]:
                _add(f"{word.capitalize()}{num}")

        # --- Two-word combos (critical + high only) ---
        combo_words = (critical + high)[:15]
        for w1, w2 in itertools.permutations(combo_words[:10], 2):
            for sep in pref_seps[:3]:
                combo = f"{w1.capitalize()}{sep}{w2.capitalize()}"
                _add(combo)
                for num in nums[:5]:
                    _add(f"{combo}{num}")
            camel = f"{w1.capitalize()}{w2.capitalize()}"
            _add(camel)
            for num in nums[:3]:
                _add(f"{camel}{num}")
            if leet_pct >= 0.2:
                for lv in leet_variants(camel, max_variants=3):
                    _add(lv)

        # --- Glue word combos ---
        if GLUE_WORDS:
            for word in combo_words[:10]:
                for glue in GLUE_WORDS[:6]:
                    base = f"{word.capitalize()}{glue.capitalize()}"
                    _add(base)
                    for num in nums[:4]:
                        _add(f"{base}{num}")
                    for sep in pref_seps[:2]:
                        sg = f"{word.capitalize()}{sep}{glue.capitalize()}"
                        _add(sg)
                        for num in nums[:3]:
                            _add(f"{sg}{num}")

        # --- Email/username based ---
        for email in EMAILS:
            user = email.split("@")[0]
            _add(user)
            for num in nums[:5]:
                _add(f"{user}{num}")

        for uname in USERNAMES:
            for variant in case_variants(uname):
                _add(variant)
                for num in nums[:5]:
                    _add(f"{variant}{num}")

        scored.sort(key=lambda x: x[0], reverse=True)
        return scored
''')


# ============================================================================
# HELPER FUNCTIONS — shared by generate_script and generate_script_llm
# ============================================================================

def _classify_words_into_tiers(
    profile: TargetProfile,
    analysis: PatternAnalysis,
) -> dict[str, list[str]]:
    """Classify profile words into priority tiers for generation."""
    critical: list[str] = []
    high: list[str] = []
    medium: list[str] = []
    low: list[str] = []
    seen: set[str] = set()

    skip_words = {"none", "n/a", "no", "unknown", "fake"}

    def _add(word: str, tier: list[str]) -> None:
        if not word or " " in word:
            return
        if word.replace(".", "").isdigit():
            return
        key = word.lower()
        if key in seen or key in skip_words:
            return
        seen.add(key)
        tier.append(word)

    from xora.password_profiler import deleet_to_words
    password_word_counts: Counter = Counter()
    for pw in profile.known_passwords:
        pw_words_unique = {w.lower() for w in deleet_to_words(pw) if len(w) >= 3}
        for w in pw_words_unique:
            password_word_counts[w] += 1

    for val in [profile.first_name, profile.last_name]:
        if val:
            for part in val.split():
                _add(part, high)
    if profile.partner_name:
        for part in profile.partner_name.split():
            _add(part, critical)
    for pet in profile.pet_names:
        _add(pet, critical)
    for nick in profile.nicknames:
        _add(nick, critical)
    for child in profile.children_names:
        if child and child.lower() not in ("none", "n/a", "no", "unknown"):
            _add(child.split()[0], critical)

    for item in analysis.inferred_data:
        word = item.get("word", "")
        conf = item.get("confidence", 0)
        if not word or " " in word:
            continue
        if conf >= 0.7:
            _add(word, critical)
        elif conf >= 0.5:
            _add(word, high)
        elif conf >= 0.3:
            _add(word, medium)
        else:
            _add(word, low)

    for corr in analysis.correlation_data:
        for sw in corr.get("suggested_words", []):
            if sw and " " not in sw:
                _add(sw, high)

    interest_set = {w.lower() for w in (profile.interests + profile.companies)}
    all_base = profile.all_base_words()
    for word in all_base:
        if word.lower() in seen:
            continue
        pw_count = password_word_counts.get(word.lower(), 0)
        if pw_count >= 2:
            _add(word, critical)
        elif pw_count == 1:
            _add(word, high)
        elif word.lower() in interest_set:
            _add(word, high)
        else:
            _add(word, low)

    return {
        "critical": critical,
        "high": high,
        "medium": medium,
        "low": low,
    }


def _extract_leet_map(
    known_passwords: list[str],
    preferred_seps: list[str] | None = None,
) -> dict[str, list[str]]:
    """Build a leet map from the target's actual substitution patterns."""
    sep_set = set(preferred_seps or [])

    leet_to_alpha = {
        "0": "o", "3": "e", "4": "a", "5": "s", "7": "t",
        "8": "b", "9": "g", "@": "a", "$": "s", "!": "i",
        "1": "i",
    }
    sub_counts: dict[str, dict[str, int]] = {}

    for pw in known_passwords:
        chars = list(pw)
        n = len(chars)
        for idx, c in enumerate(chars):
            if c not in leet_to_alpha:
                continue
            has_alpha_neighbor = False
            if idx > 0 and chars[idx - 1].isalpha():
                has_alpha_neighbor = True
            if idx < n - 1 and chars[idx + 1].isalpha():
                has_alpha_neighbor = True
            if not has_alpha_neighbor:
                continue
            alpha = leet_to_alpha[c]
            if alpha not in sub_counts:
                sub_counts[alpha] = {}
            sub_counts[alpha][c] = sub_counts[alpha].get(c, 0) + 1

    if not sub_counts:
        return {
            "a": ["4", "@"], "e": ["3"], "i": ["1", "!"],
            "o": ["0"], "s": ["5", "$"], "t": ["7"],
            "l": ["1"], "b": ["8"], "g": ["9"],
        }

    result: dict[str, list[str]] = {}
    for alpha, subs in sub_counts.items():
        def _sort_key(s: str) -> tuple[int, int]:
            is_sep = 1 if s in sep_set else 0
            return (is_sep, -subs[s])
        ordered = sorted(subs.keys(), key=_sort_key)
        result[alpha] = ordered

    return result


def _extract_separator_fingerprint(
    analysis: PatternAnalysis,
) -> tuple[list[str], list[str]]:
    """Extract preferred and rare separators from correlation data."""
    preferred: list[str] = []
    rare: list[str] = []

    for corr in analysis.correlation_data:
        if corr.get("pattern_name") != "separator_fingerprint":
            continue
        evidence = corr.get("evidence", [])
        for entry in evidence:
            m = re.match(r"'(.+?)' used (\d+)x \((\d+)%\)", entry)
            if m:
                char = m.group(1)
                pct = int(m.group(3))
                if pct >= 15:
                    preferred.append(char)
                else:
                    rare.append(char)
        if preferred:
            return preferred, rare

    if analysis.preferred_specials:
        return analysis.preferred_specials[:4], analysis.preferred_specials[4:]

    return ["!", "_", "#"], ["&"]


def _enrich_numbers(
    profile: TargetProfile,
    analysis: PatternAnalysis,
) -> list[str]:
    """Merge profile numbers with inferred birthday patterns, graduation years,
    and correlation-suggested years.
    """
    nums = list(profile.all_numbers())
    seen = set(nums)

    for item in analysis.inferred_data:
        rule = item.get("rule", "")
        word = item.get("word", "")
        if rule in ("birthday_pattern", "graduation_year") and word:
            if word not in seen and word.replace(".", "").isdigit():
                seen.add(word)
                nums.append(word)

    for corr in analysis.correlation_data:
        for sw in corr.get("suggested_words", []):
            if sw and sw.isdigit() and sw not in seen:
                seen.add(sw)
                nums.append(sw)

    return nums


def _build_data_constants(
    profile: TargetProfile,
    analysis: PatternAnalysis,
    *,
    target_name: str,
    llm_model: str = "none",
    generation_mode: str = "fallback",
    llm_candidates: list[str] | None = None,
    min_length: int = 8,
    max_length: int = 64,
    require_upper: bool = False,
    require_lower: bool = False,
    require_digit: bool = False,
    require_special: bool = False,
    custom_specials: str | None = None,
    leet_override: float | None = None,
    leet_exhaustive: bool = False,
) -> dict:
    """Build the data dictionary used to fill the script skeleton."""
    tiers = _classify_words_into_tiers(profile, analysis)
    numbers = _enrich_numbers(profile, analysis)
    preferred_seps, rare_seps = _extract_separator_fingerprint(analysis)
    if custom_specials:
        preferred_seps = list(custom_specials)
        rare_seps = []

    cat_weights = {}
    if analysis.password_profile:
        cat_weights = analysis.password_profile.get("priority_weights", {})
        if not cat_weights:
            cat_weights = analysis.password_profile.get("category_distribution", {})

    sp = analysis.strength_profile or {}
    sem = analysis.semantic_analysis or {}
    glue_words = sem.get("glue_words", [])
    semantic_templates = sem.get("semantic_templates", [])
    role_vocabulary = sem.get("role_vocabulary", {})
    leet_map = _extract_leet_map(profile.known_passwords, preferred_seps)

    return {
        "target_name": target_name,
        "version": "0.2.0",
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "script_name": "generate_passwords.py",
        "generation_mode": generation_mode,
        "word_tiers": json.dumps(tiers, indent=4),
        "numbers": json.dumps(numbers, indent=4),
        "known_passwords": json.dumps(profile.known_passwords, indent=4),
        "emails": json.dumps(profile.emails, indent=4),
        "usernames": json.dumps(profile.usernames, indent=4),
        "cap_style": analysis.capitalization_style,
        "cap_patterns": json.dumps(analysis.cap_patterns, indent=4),
        "avg_length": round(analysis.avg_length, 1),
        "pattern_templates": json.dumps(analysis.unique_templates, indent=4),
        "category_weights": json.dumps(cat_weights, indent=4),
        "preferred_separators": json.dumps(preferred_seps),
        "rare_separators": json.dumps(rare_seps),
        "strength_tier": sp.get("tier", "unknown"),
        "strength_avg_score": round(sp.get("avg_score", 0.5), 2),
        "strength_reuse_ratio": round(sp.get("reuse_ratio", 0.0), 2),
        "strength_leet_pct": round(leet_override if leet_override is not None else sp.get("leet_usage_pct", 0.0), 2),
        "leet_exhaustive": leet_exhaustive,
        "strength_always_special": sp.get("always_has_special", False),
        "strength_always_digit": sp.get("always_has_digit", False),
        "strength_always_upper": sp.get("always_has_upper", False),
        "llm_candidates": json.dumps(llm_candidates or [], indent=4),
        "llm_model": llm_model,
        "leet_map": json.dumps(leet_map, indent=4),
        "glue_words": json.dumps(glue_words, indent=4),
        "semantic_templates": json.dumps(semantic_templates, indent=4),
        "role_vocabulary": json.dumps(role_vocabulary, indent=4),
        "min_length": min_length,
        "max_length": max_length,
        "require_upper": require_upper,
        "require_lower": require_lower,
        "require_digit": require_digit,
        "require_special": require_special,
        "derivation_chains": json.dumps(
            [c.to_dict() for c in analysis.derivation_chains], indent=4
        ),
    }


# ============================================================================
# SCRIPT GENERATION
# ============================================================================

def generate_script(
    profile: TargetProfile,
    analysis: PatternAnalysis,
    *,
    target_name: str,
    llm_model: str = "none",
    llm_candidates: list[str] | None = None,
    min_length: int = 8,
    max_length: int = 64,
    require_upper: bool = False,
    require_lower: bool = False,
    require_digit: bool = False,
    require_special: bool = False,
    custom_specials: str | None = None,
    leet_override: float | None = None,
    leet_exhaustive: bool = False,
) -> str:
    """Generate the standalone password script.

    The combinatorial engine is always xora's own reliable template engine.
    When an LLM is available, psychologically-targeted candidate strings are
    injected into LLM_CANDIDATES so the engine seeds them at the top of the
    ranked output — no LLM-written Python code involved.
    """
    mode = f"LLM-enhanced ({llm_model})" if llm_candidates else "no LLM"
    data = _build_data_constants(
        profile, analysis,
        target_name=target_name,
        llm_model=llm_model,
        generation_mode=mode,
        llm_candidates=llm_candidates,
        min_length=min_length,
        max_length=max_length,
        require_upper=require_upper,
        require_lower=require_lower,
        require_digit=require_digit,
        require_special=require_special,
        custom_specials=custom_specials,
        leet_override=leet_override,
        leet_exhaustive=leet_exhaustive,
    )
    data["generation_engine"] = _FALLBACK_ENGINE
    return _SCRIPT_SKELETON.format(**data)


def generate_script_llm(
    profile: TargetProfile,
    analysis: PatternAnalysis,
    *,
    target_name: str,
    llm_model: str = "none",
    llm_candidates: list[str] | None = None,
    custom_code: str | None = None,
    min_length: int = 8,
    max_length: int = 64,
    require_upper: bool = False,
    require_lower: bool = False,
    require_digit: bool = False,
    require_special: bool = False,
    custom_specials: str | None = None,
    leet_override: float | None = None,
    leet_exhaustive: bool = False,
) -> str:
    """Generate script with LLM-written engine code + targeted candidates.

    When ``custom_code`` is provided (and contains a valid generate_all),
    it replaces the fallback engine. ``llm_candidates`` are always injected
    into LLM_CANDIDATES regardless.
    """
    data = _build_data_constants(
        profile, analysis,
        target_name=target_name,
        llm_model=llm_model,
        generation_mode=f"LLM-driven ({llm_model})",
        llm_candidates=llm_candidates,
        min_length=min_length,
        max_length=max_length,
        require_upper=require_upper,
        require_lower=require_lower,
        require_digit=require_digit,
        require_special=require_special,
        custom_specials=custom_specials,
        leet_override=leet_override,
        leet_exhaustive=leet_exhaustive,
    )
    if custom_code and "def generate_all" in custom_code:
        data["generation_engine"] = custom_code
    else:
        data["generation_engine"] = _FALLBACK_ENGINE
    return _SCRIPT_SKELETON.format(**data)


def build_intelligence_summary(
    profile: TargetProfile,
    analysis: PatternAnalysis,
) -> dict:
    """Build a compact intelligence summary for LLM targeted candidate generation.

    This is the data package the LLM receives to generate psychologically-
    targeted password candidates. The LLM returns a JSON list of strings;
    xora's own template engine handles all combinatorial expansion.
    """
    tiers = _classify_words_into_tiers(profile, analysis)
    preferred_seps, rare_seps = _extract_separator_fingerprint(analysis)
    sp = analysis.strength_profile or {}
    sem = analysis.semantic_analysis or {}

    return {
        "word_tiers": tiers,
        "numbers": _enrich_numbers(profile, analysis),
        "known_passwords": profile.known_passwords,
        "cap_style": analysis.capitalization_style,
        "cap_patterns": analysis.cap_patterns,
        "avg_length": round(analysis.avg_length, 1),
        "pattern_templates": analysis.unique_templates,
        "preferred_separators": preferred_seps,
        "rare_separators": rare_seps,
        "strength_tier": sp.get("tier", "unknown"),
        "leet_usage_pct": sp.get("leet_usage_pct", 0.0),
        "avg_score": sp.get("avg_score", 0.5),
        "glue_words": sem.get("glue_words", []),
        "semantic_templates": sem.get("semantic_templates", []),
        "role_vocabulary": sem.get("role_vocabulary", {}),
        "leet_map": _extract_leet_map(profile.known_passwords, preferred_seps),
        "category_weights": (
            analysis.password_profile.get("priority_weights", {})
            if analysis.password_profile else {}
        ),
        "derivation_chains": [c.to_dict() for c in analysis.derivation_chains],
    }


# ============================================================================
# WRITE TARGET FOLDER — creates all artifacts
# ============================================================================

def write_target_folder(
    target_dir: Path,
    profile: TargetProfile,
    analysis: PatternAnalysis,
    *,
    target_name: str,
    llm_model: str = "none",
    llm_candidates: list[str] | None = None,
    custom_code: str | None = None,
    raw_text: str = "",
    min_length: int = 8,
    max_length: int = 64,
    require_upper: bool = False,
    require_lower: bool = False,
    require_digit: bool = False,
    require_special: bool = False,
    custom_specials: str | None = None,
    cached_steps: list[str] | None = None,
    leet_override: float | None = None,
    leet_exhaustive: bool = False,
) -> Path:
    """Create the target folder with all artifacts."""
    target_dir.mkdir(parents=True, exist_ok=True)

    # Raw profile
    raw_text_final = raw_text or profile.raw_text
    raw_path = target_dir / "profile.raw"
    raw_path.write_text(raw_text_final, encoding="utf-8")

    # Cache metadata for subsequent `xora analyze` runs
    profile_hash = hashlib.sha256(raw_text_final.encode()).hexdigest()
    all_steps = cached_steps or [
        "parse", "categorize", "semantics", "inference",
        "correlations", "curate", "targeted_candidates",
    ]
    cache_meta = {
        "profile_hash": profile_hash,
        "llm_model": llm_model,
        "timestamp": datetime.now().isoformat(),
        "cached_steps": all_steps,
    }
    cache_path = target_dir / "cache_meta.json"
    cache_path.write_text(json.dumps(cache_meta, indent=2), encoding="utf-8")

    # Parsed profile JSON
    parsed_path = target_dir / "profile.parsed.json"
    parsed_path.write_text(
        json.dumps(profile.to_dict(), indent=2), encoding="utf-8"
    )

    # Pattern analysis
    analysis_path = target_dir / "analysis.json"
    analysis_path.write_text(
        json.dumps(analysis.to_dict(), indent=2), encoding="utf-8"
    )

    # Build tier info for the report
    tiers = _classify_words_into_tiers(profile, analysis)

    # Human-readable analysis report
    report_lines = _build_report(
        profile, analysis, tiers, target_name, llm_model,
    )
    report_path = target_dir / "analysis.md"
    report_path.write_text("\n".join(report_lines) + "\n", encoding="utf-8")

    # The main artifact: standalone generation script.
    # custom_code (reviewed LLM engine) is used when present; otherwise falls
    # back to xora's own template engine. llm_candidates are always injected.
    script = generate_script_llm(
        profile, analysis,
        target_name=target_name,
        llm_model=llm_model,
        llm_candidates=llm_candidates,
        custom_code=custom_code,
        min_length=min_length,
        max_length=max_length,
        require_upper=require_upper,
        require_lower=require_lower,
        require_digit=require_digit,
        require_special=require_special,
        custom_specials=custom_specials,
        leet_override=leet_override,
        leet_exhaustive=leet_exhaustive,
    )
    script_path = target_dir / "generate_passwords.py"
    script_path.write_text(script, encoding="utf-8")
    script_path.chmod(0o755)

    return target_dir


def write_analysis_files(
    target_dir: Path,
    profile: TargetProfile,
    analysis: PatternAnalysis,
    *,
    target_name: str,
    llm_model: str = "none",
    raw_text: str = "",
    cached_steps: list[str] | None = None,
) -> Path:
    """Write analysis artifacts (profile, analysis.json, analysis.md) WITHOUT codegen.

    Called at the end of the analysis phase so files are available for the
    interactive review session before generate_passwords.py is written.
    """
    target_dir.mkdir(parents=True, exist_ok=True)

    raw_text_final = raw_text or profile.raw_text
    (target_dir / "profile.raw").write_text(raw_text_final, encoding="utf-8")

    profile_hash = hashlib.sha256(raw_text_final.encode()).hexdigest()
    all_steps = cached_steps or [
        "parse", "categorize", "semantics", "inference",
        "correlations", "curate",
    ]
    cache_meta = {
        "profile_hash": profile_hash,
        "llm_model": llm_model,
        "timestamp": datetime.now().isoformat(),
        "cached_steps": all_steps,
    }
    (target_dir / "cache_meta.json").write_text(
        json.dumps(cache_meta, indent=2), encoding="utf-8"
    )

    (target_dir / "profile.parsed.json").write_text(
        json.dumps(profile.to_dict(), indent=2), encoding="utf-8"
    )

    (target_dir / "analysis.json").write_text(
        json.dumps(analysis.to_dict(), indent=2), encoding="utf-8"
    )

    tiers = _classify_words_into_tiers(profile, analysis)
    report_lines = _build_report(profile, analysis, tiers, target_name, llm_model)
    (target_dir / "analysis.md").write_text("\n".join(report_lines) + "\n", encoding="utf-8")

    return target_dir


def _build_report(
    profile: TargetProfile,
    analysis: PatternAnalysis,
    tiers: dict,
    target_name: str,
    llm_model: str,
) -> list[str]:
    """Build the human-readable analysis.md report."""
    report_lines = [
        f"# xora Analysis Report: {target_name}",
        f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        "",
        "## Word Tiers",
    ]
    for tier_name in ["critical", "high", "medium", "low"]:
        words = tiers.get(tier_name, [])
        report_lines.append(f"  **{tier_name}** ({len(words)}): {', '.join(words) or '(none)'}")

    report_lines.extend([
        "",
        "## Numbers",
        ", ".join(_enrich_numbers(profile, analysis)) or "(none extracted)",
        "",
        "## Known Passwords",
    ])
    if profile.known_passwords:
        for pw in profile.known_passwords:
            report_lines.append(f"  - `{pw}`")
    else:
        report_lines.append("  (none)")

    report_lines.extend([
        "",
        "## Pattern Analysis",
        f"- Capitalization style: {analysis.capitalization_style}",
        f"- Number style: {analysis.number_style}",
        f"- Average password length: {analysis.avg_length:.1f}",
    ])
    preferred_seps, rare_seps = _extract_separator_fingerprint(analysis)
    report_lines.append(f"- Preferred separators: {', '.join(repr(s) for s in preferred_seps)}")
    if rare_seps:
        report_lines.append(f"- Rare separators: {', '.join(repr(s) for s in rare_seps)}")

    report_lines.extend(["", "### Detected Templates"])
    for pat in analysis.patterns:
        report_lines.append(f"  - `{pat.source}` → `{pat.template}` ({pat.description})")
    if not analysis.patterns:
        report_lines.append("  (no known passwords to analyze)")

    pw_prof = analysis.password_profile
    if pw_prof and pw_prof.get("categorized_passwords"):
        report_lines.extend(["", "## Password Psychology Profile"])
        cat_dist = pw_prof.get("category_distribution", {})
        if cat_dist:
            report_lines.extend(["", "### Category Distribution"])
            for cat, pct in cat_dist.items():
                bar = "█" * int(pct * 40)
                report_lines.append(f"  {cat:<16} {pct:>5.1%}  {bar}")
        top = pw_prof.get("top_categories", [])
        if top:
            report_lines.extend(["", f"**Primary themes:** {', '.join(top[:5])}"])
        report_lines.extend(["", "### Decoded Passwords"])
        for cp in pw_prof["categorized_passwords"]:
            conf = cp.get("confidence", 0)
            reasoning = cp.get("reasoning", "")
            reason_str = f" — {reasoning}" if reasoning else ""
            report_lines.append(
                f"  - `{cp['original']}` → `{cp['decoded']}` "
                f"[{cp['category']}] ({conf:.0%}){reason_str}"
            )
        prio = pw_prof.get("priority_weights")
        if prio:
            report_lines.extend(["", "### Generation Priority Weights"])
            for cat, w in prio.items():
                report_lines.append(f"  {cat:<16} {w:.1%}")

    sp = analysis.strength_profile
    if sp:
        report_lines.extend(["", "## Password Strength Assessment", ""])
        report_lines.append(
            f"**Overall: {sp.get('tier', 'unknown').upper()}** "
            f"(avg score: {sp.get('avg_score', 0):.2f}/1.00)"
        )
        report_lines.extend([
            "",
            "| Metric | Value |",
            "|--------|-------|",
            f"| Avg length | {sp.get('avg_length', 0):.1f} |",
            f"| Length range | {sp.get('min_length', 0)}-{sp.get('max_length', 0)} |",
            f"| Avg char classes | {sp.get('avg_char_classes', 0):.1f}/4 |",
            f"| Avg entropy | {sp.get('avg_entropy', 0):.0f} bits |",
            f"| Leet speak usage | {sp.get('leet_usage_pct', 0):.0%} |",
            f"| Word reuse | {sp.get('reuse_ratio', 0):.0%} |",
        ])

        indiv = sp.get("individual", [])
        if indiv:
            report_lines.extend(["", "### Per-Password Breakdown", ""])
            report_lines.append("| Password | Score | Tier | Length | Classes | Entropy |")
            report_lines.append("|----------|-------|------|--------|---------|---------|")
            for s in indiv:
                report_lines.append(
                    f"| `{s['password']}` | {s['score']:.2f} | {s['tier']} "
                    f"| {s['length']} | {s['char_classes']}/4 "
                    f"| {s['entropy_bits']:.0f} |"
                )

    if analysis.inferred_data:
        report_lines.extend(["", "## Inferred Intelligence",
                             "", "Words derived from profile data by reasoning about context:"])
        rules_seen: dict[str, list] = {}
        for item in analysis.inferred_data:
            rule = item.get("rule", "unknown")
            rules_seen.setdefault(rule, []).append(item)
        for rule, items in rules_seen.items():
            report_lines.append(f"\n### {rule.replace('_', ' ').title()}")
            for item in items:
                conf = item.get("confidence", 0)
                report_lines.append(
                    f"  - **{item.get('word', '')}** ({conf:.0%}) "
                    f"— {item.get('reasoning', '')}"
                )

    if analysis.correlation_data:
        report_lines.extend(["", "## Correlation Insights",
                             "", "Hidden connections between passwords and profile data:"])
        for corr in analysis.correlation_data:
            name = corr.get("pattern_name", "")
            report_lines.append(f"\n### {name.replace('_', ' ').title()}")
            report_lines.append(f"**Insight:** {corr.get('insight', '')}")
            conf = corr.get("confidence", 0)
            report_lines.append(f"**Confidence:** {conf:.0%}")
            evidence = corr.get("evidence", [])
            if evidence:
                report_lines.append("**Evidence:**")
                for e in evidence:
                    report_lines.append(f"  - {e}")
            suggested = corr.get("suggested_words", [])
            if suggested:
                report_lines.append(
                    f"**Suggested words:** {', '.join(suggested)}"
                )

    sem = analysis.semantic_analysis or {}
    sem_passwords = sem.get("passwords", [])
    sem_glue = sem.get("glue_words", [])
    sem_templates = sem.get("semantic_templates", [])
    sem_roles = sem.get("role_vocabulary", {})
    if sem_passwords:
        report_lines.extend([
            "",
            "## Semantic Decomposition",
            "",
            "Each password broken down by *meaning*, not just structure:",
            "",
        ])
        for sp_item in sem_passwords[:15]:
            comps = sp_item.get("components", [])
            comp_strs = [
                f"**{c['role']}**: `{c['value']}`" for c in comps
            ]
            report_lines.append(
                f"- `{sp_item['original']}` → {' + '.join(comp_strs)}"
            )
            report_lines.append(
                f"  - Template: `{sp_item.get('semantic_template', '')}`"
                f" | Category: {sp_item.get('category', 'unknown')}"
            )
        if len(sem_passwords) > 15:
            report_lines.append(f"- ... and {len(sem_passwords) - 15} more")
        if sem_glue:
            report_lines.extend([
                "", "### Glue Words", "",
                f"**Detected:** {', '.join(f'`{g}`' for g in sem_glue)}",
            ])
        if sem_templates:
            report_lines.extend(["", "### Semantic Templates", ""])
            for tmpl in sem_templates[:10]:
                report_lines.append(f"- `{tmpl}`")
        if sem_roles:
            report_lines.extend(["", "### Role Vocabulary", ""])
            for role, words in sem_roles.items():
                report_lines.append(
                    f"- **{role}**: {', '.join(f'`{w}`' for w in words[:8])}"
                    f"{'...' if len(words) > 8 else ''}"
                )

    return report_lines
